{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"asg5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP/vUzUuNcM+HWCr3ddXm9U"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JKzFdNfMdYk-"},"source":["# Assignment 5\n","\n","## TOC:\n","\n","* [1. Define some functions](#1)\n","* [2. Load training data](#2)\n","* [3. Define text classification algorithm](#3)\n","* [4. Prepare training and validation data](#4)\n","* [5. Train the model](#5)\n","* [6. Make prediction](#6)\n","\n","In this assignment, you will be asked to perform text classification using `torchtext`."]},{"cell_type":"code","metadata":{"id":"XT9ziUVrdUa5","executionInfo":{"status":"ok","timestamp":1597174061629,"user_tz":300,"elapsed":68210,"user":{"displayName":"Yi Zuo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhReKKZwVjcYzd5a8W2g_vrK1s0tWsEI9KWq7MzkQ=s64","userId":"09383670044492678123"}},"outputId":"31b5d1ba-fae3-4808-a94b-bc47b71bd334","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rioHoniLd1JV"},"source":["%cd gdrive/My\\ Drive/Colab\\ Notebooks/hw5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SoSPl_tddfym"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIidUE1_uH6N"},"source":["Upgrade `torchtext` version"]},{"cell_type":"code","metadata":{"id":"fRvz2qhdBLIu"},"source":["!pip install --upgrade torchtext"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8z28T0ohv-mh"},"source":["You may need to restart runtime to load the upgraded `torchtext`.  "]},{"cell_type":"code","metadata":{"id":"WSro5RAudiuD"},"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import time\n","from torch.utils.data.dataset import random_split\n","import io\n","from torchtext.data.utils import ngrams_iterator, get_tokenizer\n","from torchtext.utils import unicode_csv_reader\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.vocab import Vocab\n","from tqdm import tqdm\n","import logging"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWM4drk2d-ZI"},"source":["## Define some functions <a name=\"1\"></a>"]},{"cell_type":"code","metadata":{"id":"60JrMUgHdj34"},"source":["\n","def _csv_iterator(data_path, ngrams, yield_cls=False):\n","    tokenizer = get_tokenizer(\"basic_english\")\n","    with io.open(data_path, encoding=\"utf8\") as f:\n","        reader = unicode_csv_reader(f)\n","        for row in reader:\n","            tokens = ' '.join(row[1:])\n","            tokens = tokenizer(tokens)\n","            if yield_cls:\n","                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n","            else:\n","                yield ngrams_iterator(tokens, ngrams)\n","\n","\n","def _create_data_from_iterator(vocab, iterator, include_unk):\n","    data = []\n","    labels = []\n","    with tqdm(unit_scale=0, unit='lines') as t:\n","        for cls, tokens in iterator:\n","            if include_unk:\n","                tokens = torch.tensor([vocab[token] for token in tokens])\n","            else:\n","                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n","                                        for token in tokens]))\n","                tokens = torch.tensor(token_ids)\n","            if len(tokens) == 0:\n","                logging.info('Row contains no tokens.')\n","            data.append((cls, tokens))\n","            labels.append(cls)\n","            t.update(1)\n","    return data, set(labels)\n","\n","class TextClassificationDataset(torch.utils.data.Dataset):\n","    \"\"\"Defines an abstract text classification datasets.\n","       Currently, we only support the following datasets:\n","             - AG_NEWS\n","             - SogouNews\n","             - DBpedia\n","             - YelpReviewPolarity\n","             - YelpReviewFull\n","             - YahooAnswers\n","             - AmazonReviewPolarity\n","             - AmazonReviewFull\n","    \"\"\"\n","\n","    def __init__(self, vocab, data, labels):\n","        \"\"\"Initiate text-classification dataset.\n","        Arguments:\n","            vocab: Vocabulary object used for dataset.\n","            data: a list of label/tokens tuple. tokens are a tensor after\n","                numericalizing the string tokens. label is an integer.\n","                [(label1, tokens1), (label2, tokens2), (label2, tokens3)]\n","            label: a set of the labels.\n","                {label1, label2}\n","        Examples:\n","            See the examples in examples/text_classification/\n","        \"\"\"\n","\n","        super(TextClassificationDataset, self).__init__()\n","        self._data = data\n","        self._labels = labels\n","        self._vocab = vocab\n","\n","    def __getitem__(self, i):\n","        return self._data[i]\n","\n","    def __len__(self):\n","        return len(self._data)\n","\n","    def __iter__(self):\n","        for x in self._data:\n","            yield x\n","\n","    def get_labels(self):\n","        return self._labels\n","\n","    def get_vocab(self):\n","        return self._vocab\n","\n","\n","def _setup_datasets( ngrams=1, vocab=None, include_unk=False):\n","\n","    train_csv_path = 'data/train.csv'\n","\n","    if vocab is None:\n","        logging.info('Building Vocab based on {}'.format(train_csv_path))\n","        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n","    else:\n","        if not isinstance(vocab, Vocab):\n","            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n","    logging.info('Vocab has {} entries'.format(len(vocab)))\n","    logging.info('Creating training data')\n","    train_data, train_labels = _create_data_from_iterator(\n","        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n","\n","    return (TextClassificationDataset(vocab, train_data, train_labels) )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xl6bPfPxeFby"},"source":["## Load training data <a name=\"2\"></a>"]},{"cell_type":"code","metadata":{"id":"hvoPIJLEeA9q","executionInfo":{"status":"ok","timestamp":1597174590020,"user_tz":300,"elapsed":39312,"user":{"displayName":"Yi Zuo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhReKKZwVjcYzd5a8W2g_vrK1s0tWsEI9KWq7MzkQ=s64","userId":"09383670044492678123"}},"outputId":"3838fb2d-1823-4c70-a033-1699a6953946","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["NGRAMS = 2\n","\n","train_dataset = _setup_datasets(ngrams=NGRAMS, vocab=None)\n","\n","BATCH_SIZE = 16\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["120000lines [00:10, 11158.03lines/s]\n","120000lines [00:24, 4973.35lines/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"-XGNUiQoezmo"},"source":["## Define text classification algorithm <a name=\"3\"></a>"]},{"cell_type":"code","metadata":{"id":"ei1sFf5heHxJ"},"source":["# define class\n","class TextSentiment(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)\n","\n","VOCAB_SIZE = len(train_dataset.get_vocab())\n","EMBED_DIM = 32\n","NUN_CLASS = len(train_dataset.get_labels())\n","model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n","\n","def generate_batch(batch):\n","    label = torch.tensor([entry[0] for entry in batch]) # 114000\n","    text = [entry[1] for entry in batch] # length: 114000\n","    offsets = [0] + [len(entry) for entry in text] # 114001\n","    # torch.Tensor.cumsum returns the cumulative sum\n","    # of elements in the dimension dim.\n","    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n","\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text = torch.cat(text) # 9752656\n","    return text, offsets, label\n","\n","def train_func(sub_train_):\n","\n","    # Train the model\n","    train_loss = 0\n","    train_acc = 0\n","    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n","                      collate_fn=generate_batch)\n","    for i, (text, offsets, cls) in enumerate(data):\n","        optimizer.zero_grad()\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        output = model(text, offsets)\n","        loss = criterion(output, cls)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(1) == cls).sum().item()\n","\n","    # Adjust the learning rate\n","    scheduler.step()\n","\n","    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n","\n","def test(data_):\n","    loss = 0\n","    acc = 0\n","    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    for text, offsets, cls in data:\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        model.eval()\n","        with torch.no_grad():\n","            output = model(text, offsets)\n","            loss = criterion(output, cls)\n","            loss += loss.item()\n","            acc += (output.argmax(1) == cls).sum().item()\n","\n","    return loss / len(data_), acc / len(data_)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdLV19z-e8Ls"},"source":["## Prepare training and validation data <a name=\"4\"></a>"]},{"cell_type":"code","metadata":{"id":"lGmiiyHVe4IP"},"source":["N_EPOCHS = 5\n","min_valid_loss = float('inf')\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","train_len = int(len(train_dataset) * 0.95)\n","sub_train_, sub_valid_ = \\\n","    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ut8nP9RfDNm"},"source":["## Train the model <a name=\"5\"></a>"]},{"cell_type":"code","metadata":{"id":"A45wyUK8e38U","executionInfo":{"status":"ok","timestamp":1597174772163,"user_tz":300,"elapsed":114343,"user":{"displayName":"Yi Zuo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhReKKZwVjcYzd5a8W2g_vrK1s0tWsEI9KWq7MzkQ=s64","userId":"09383670044492678123"}},"outputId":"22e9af41-50ea-4370-c020-66ad2b15ce54","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(sub_train_)\n","    valid_loss, valid_acc = test(sub_valid_)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1  | time in 0 minutes, 23 seconds\n","\tLoss: 0.0260(train)\t|\tAcc: 84.9%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 90.3%(valid)\n","Epoch: 2  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0119(train)\t|\tAcc: 93.7%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 91.5%(valid)\n","Epoch: 3  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0069(train)\t|\tAcc: 96.4%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 90.6%(valid)\n","Epoch: 4  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0038(train)\t|\tAcc: 98.2%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 91.2%(valid)\n","Epoch: 5  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 90.9%(valid)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"koK-1t27fJgi"},"source":["## Make prediction <a name=\"6\"></a>"]},{"cell_type":"code","metadata":{"id":"fTqEuUwQfF21"},"source":["# prediction\n","ag_news_label = {1 : \"World\",\n","                 2 : \"Sports\",\n","                 3 : \"Business\",\n","                 4 : \"Sci/Tec\"}\n","\n","def predict(text, model, vocab, ngrams):\n","    tokenizer = get_tokenizer(\"basic_english\")\n","    model.eval()\n","    with torch.no_grad():\n","        text = torch.tensor([vocab[token]\n","                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAKT1cAyfFpW"},"source":["# read in test set\n","test_set = pd.read_csv(\"data/test.csv\",header=None)\n","vocab = train_dataset.get_vocab()\n","\n","prediction = []\n","for piece in test_set.iloc[:,0]:\n","    prediction.append( predict(piece, model, vocab, 2))\n","\n","out = pd.DataFrame({'prediction':prediction})\n","\n","out.to_csv(\"data/sample_output.csv\",index=False)"],"execution_count":null,"outputs":[]}]}